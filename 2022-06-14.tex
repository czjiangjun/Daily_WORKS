\documentclass[10pt, oneside, a4paper]{article}      % Specifies the document class
%\documentclass[10pt, twoside, a4paper]{article}      % Specifies the document class

%%%%%%%%%%%%%%%%% CJK 中文版面控制  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{CJK} % CTEX-CJK 中文支持                            %
\usepackage{xeCJK} % seperate the english and chinese		 %
\usepackage{CJKutf8} % Texlive 中文支持                         %
\usepackage{CJKnumb} %中文序号                                   %
\usepackage{indentfirst} % 中文段落首行缩进                      %
%\setlength\parindent{22pt}       % 段落起始缩进量               %
\renewcommand{\baselinestretch}{1.2} % 中文行间距调整            %
\setlength{\textwidth}{16cm}                                     %
\setlength{\textheight}{24cm}                                    %
\setlength{\topmargin}{-1cm}                                     %
\setlength{\oddsidemargin}{0.1cm}                                %
\setlength{\evensidemargin}{\oddsidemargin}                      %
\usepackage{fancyhdr}           %使用页眉-页脚                   %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{authblk}					 %作者地址和E-mail
\usepackage{amsmath,amsthm,amsfonts,amssymb,bm}          %数学公式
\usepackage{mathrsfs}                                    %英文花体
\usepackage{tikz}					 %绘制平面图形
%\usepackage[dvipdfmx]{movie15_dvipdfmx} %插入视频
\usepackage{xcolor}                                        %使用默认允许使用颜色
%\usepackage{hyperref} 
\usepackage{graphicx}
\usepackage{subfigure}           %图片跨页
\usepackage{animate}		 %插入动画
\usepackage{caption}
\captionsetup{font=footnotesize}

%\usepackage[version=3]{mhchem}		%化学公式
\usepackage{chemformula}
\usepackage{chemfig}		%化学公式

\usepackage{fontspec} % use to set font
\setCJKmainfont{SimSun}
\XeTeXlinebreaklocale "zh"  % Auto linebreak for chinese
\XeTeXlinebreakskip = 0pt plus 1pt % Auto linebreak for chinese

\usepackage{longtable}                                   %使用长表格
\usepackage{multirow}
\usepackage{makecell}		%允许单元格内换行

\usepackage{arydshln}
\newcommand{\adots}{\mathinner{\mkern2mu%
\raisebox{0.1em}{.}\mkern2mu\raisebox{0.4em}{.}%
\mkern2mu\raisebox{0.7em}{.}\mkern1mu}}
%%%%%%%%%%%%%%%%%%%%%%%%%  参考文献引用 %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%尽量使用 BibTeX(含有超链接，数据库的条目URL即可)                %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[numbers,sort&compress]{natbib} %紧密排列             %
\usepackage[sectionbib]{chapterbib}        %每章节单独参考文献   %
%\usepackage{footbib}			   %脚注列出参考文献　   %
\usepackage{hypernat}                                                                         %
%\usepackage[dvipdfm,bookmarksopen=true,pdfstartview=FitH,CJKbookmarks]{hyperref}              %
\usepackage[bookmarksopen=true,pdfstartview=FitH,CJKbookmarks]{hyperref}              %
\hypersetup{bookmarksnumbered,colorlinks,linkcolor=green,citecolor=blue,urlcolor=red}         %
%参考文献含有超链接引用时需要下列宏包，注意与natbib有冲突        %
%\usepackage[dvipdfm]{hyperref}                                  %
%\usepackage{hypernat}                                           %
\newcommand{\upcite}[1]{\hspace{0ex}\textsuperscript{\cite{#1}}} %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\AtBeginDvi{\special{pdf:tounicode GBK-EUC-UCS2}} %CTEX用dvipdfmx的话，用该命令可以解决      %
%						   %pdf书签的中文乱码问题		      %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%---------------------------------xeCJK下设置中文字体-----------------------------------------%  
\setCJKfamilyfont{song}{SimSun}                             %宋体 song  
\newcommand{\song}{\CJKfamily{song}}                        % 宋体   (Windows自带simsun.ttf)  
\setCJKfamilyfont{xs}{NSimSun}                              %新宋体 xs  
\newcommand{\xs}{\CJKfamily{xs}}  
\setCJKfamilyfont{fs}{FangSong_GB2312}                      %仿宋2312 fs  
\newcommand{\fs}{\CJKfamily{fs}}                            %仿宋体 (Windows自带simfs.ttf)  
\setCJKfamilyfont{kai}{KaiTi_GB2312}                        %楷体2312  kai  
\newcommand{\kai}{\CJKfamily{kai}}                            
\setCJKfamilyfont{yh}{Microsoft YaHei}                    %微软雅黑 yh  
\newcommand{\yh}{\CJKfamily{yh}}  
\setCJKfamilyfont{hei}{SimHei}                                    %黑体  hei  
\newcommand{\hei}{\CJKfamily{hei}}                          % 黑体   (Windows自带simhei.ttf)  
\setCJKfamilyfont{msunicode}{Arial Unicode MS}            %Arial Unicode MS: msunicode  
\newcommand{\msunicode}{\CJKfamily{msunicode}}  
\setCJKfamilyfont{li}{LiSu}                                            %隶书  li  
\newcommand{\li}{\CJKfamily{li}}  
\setCJKfamilyfont{yy}{YouYuan}                             %幼圆  yy  
\newcommand{\yy}{\CJKfamily{yy}}  
\setCJKfamilyfont{xm}{MingLiU}                                        %细明体  xm  
\newcommand{\xm}{\CJKfamily{xm}}  
\setCJKfamilyfont{xxm}{PMingLiU}                             %新细明体  xxm  
\newcommand{\xxm}{\CJKfamily{xxm}}  
\setCJKfamilyfont{hwsong}{STSong}                            %华文宋体  hwsong  
\newcommand{\hwsong}{\CJKfamily{hwsong}}  
\setCJKfamilyfont{hwzs}{STZhongsong}                        %华文中宋  hwzs  
\newcommand{\hwzs}{\CJKfamily{hwzs}}  
\setCJKfamilyfont{hwfs}{STFangsong}                            %华文仿宋  hwfs  
\newcommand{\hwfs}{\CJKfamily{hwfs}}  
\setCJKfamilyfont{hwxh}{STXihei}                                %华文细黑  hwxh  
\newcommand{\hwxh}{\CJKfamily{hwxh}}  
\setCJKfamilyfont{hwl}{STLiti}                                        %华文隶书  hwl  
\newcommand{\hwl}{\CJKfamily{hwl}}  
\setCJKfamilyfont{hwxw}{STXinwei}                                %华文新魏  hwxw  
\newcommand{\hwxw}{\CJKfamily{hwxw}}  
\setCJKfamilyfont{hwk}{STKaiti}                                    %华文楷体  hwk  
\newcommand{\hwk}{\CJKfamily{hwk}}  
\setCJKfamilyfont{hwxk}{STXingkai}                            %华文行楷  hwxk  
\newcommand{\hwxk}{\CJKfamily{hwxk}}  
\setCJKfamilyfont{hwcy}{STCaiyun}                                 %华文彩云 hwcy  
\newcommand{\hwcy}{\CJKfamily{hwcy}}  
\setCJKfamilyfont{hwhp}{STHupo}                                 %华文琥珀   hwhp  
\newcommand{\hwhp}{\CJKfamily{hwhp}}  
\setCJKfamilyfont{fzsong}{Simsun (Founder Extended)}     %方正宋体超大字符集   fzsong  
\newcommand{\fzsong}{\CJKfamily{fzsong}}  
\setCJKfamilyfont{fzyao}{FZYaoTi}                                    %方正姚体  fzy  
\newcommand{\fzyao}{\CJKfamily{fzyao}}  
\setCJKfamilyfont{fzshu}{FZShuTi}                                    %方正舒体 fzshu  
\newcommand{\fzshu}{\CJKfamily{fzshu}}  
\setCJKfamilyfont{asong}{Adobe Song Std}                        %Adobe 宋体  asong  
\newcommand{\asong}{\CJKfamily{asong}}  
\setCJKfamilyfont{ahei}{Adobe Heiti Std}                            %Adobe 黑体  ahei  
\newcommand{\ahei}{\CJKfamily{ahei}}  
\setCJKfamilyfont{akai}{Adobe Kaiti Std}                            %Adobe 楷体  akai  
\newcommand{\akai}{\CJKfamily{akai}}  
%------------------------------设置字体大小------------------------%  
\newcommand{\chuhao}{\fontsize{42pt}{\baselineskip}\selectfont}     %初号  
\newcommand{\xiaochuhao}{\fontsize{36pt}{\baselineskip}\selectfont} %小初号  
\newcommand{\yihao}{\fontsize{28pt}{\baselineskip}\selectfont}      %一号  
\newcommand{\erhao}{\fontsize{21pt}{\baselineskip}\selectfont}      %二号  
\newcommand{\xiaoerhao}{\fontsize{18pt}{\baselineskip}\selectfont}  %小二号  
\newcommand{\sanhao}{\fontsize{15.75pt}{\baselineskip}\selectfont}  %三号  
\newcommand{\sihao}{\fontsize{14pt}{\baselineskip}\selectfont}%     四号  
\newcommand{\xiaosihao}{\fontsize{12pt}{\baselineskip}\selectfont}  %小四号  
\newcommand{\wuhao}{\fontsize{10.5pt}{\baselineskip}\selectfont}    %五号  
\newcommand{\xiaowuhao}{\fontsize{9pt}{\baselineskip}\selectfont}   %小五号  
\newcommand{\liuhao}{\fontsize{7.875pt}{\baselineskip}\selectfont}  %六号  
\newcommand{\qihao}{\fontsize{5.25pt}{\baselineskip}\selectfont}    %七号  

%%%%%%%%%%%%%%%%%%%%%  % 插图使用位置  %%%%%%%%%%%%%%%%%%%%%%%%%%%
\graphicspath{{../Presentation_Beamer/Figures/}}                            %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{verbatim}			%Verbatim 宏包重新实现了 Verbatim 环境，并且提供一个命令可以导入一个 ASCII 文件到文档中
%\verbatiminput{filename}

%在beamer里面使用verbatim环境，可以通过在frame的参数里面添加 containsverbatim / fragile来解决，不过 containsverbatim 会导致pause失效
%\begin{frame}[containsverbatim] %也可以用 \begin{frame}[fragile]
%	\begin{verbatim}
%	\usepackage{xcolor}
%	TEST
%	\end{verbatim}
%\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 用 authblk 包 支持作者和E-mail %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\title{More than one Author with different Affiliations}				     %
\title{\hei 标题}
%\author[a]{Author A}									     %
\author[a]{Author/作者 A}   %
%\author[a]{Author B}									     %
%\author[a]{Author C \thanks{Corresponding author: email@mail.com}}			     %
\author[a]{Author/通讯作者 C \thanks{Corresponding author: cores-email@mail.com}}     %
%\author[b]{Author D}									     %
\author[b]{Author/作者 D}									     %
%\author[b]{Author E}									     %
%\affil[a]{Department of Computer Science, \LaTeX\ University}				     %
\affil[a]{作者单位-1 \authorcr 地址}    %\authorcr表示换行
%\affil[b]{Department of Mechanical Engineering, \LaTeX\ University}			     %
\affil[b]{作者单位-2}			     %
											     %
%%% 使用 \thanks 定义通讯作者								     %
%%\affil命令后的{}中的内容，如果觉得需要换行的话，换行命令是\authorcr（不是\\）。
%%Email中可以吧相同邮箱的人@前面的内容写在一个{}里，用逗号隔开。注意{和}前面要加\。例如：
%%\affil[*]{单位1, \authorcr Email: \{zuozhe1, zuozhe2\}@yahoo.com, zuozhe3@sina.com}
											     %
\renewcommand*{\Authfont}{\small\rm} % 修改作者的字体与大小				     %
\renewcommand*{\Affilfont}{\small\it} % 修改机构名称的字体与大小			     %
\renewcommand\Authands{ and } % 去掉 and 前的逗号					     %
\renewcommand\Authands{ , } % 将 and 换成逗号					     %
\date{} % 去掉日期									     %
%\date{2020-12-30}									     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%  % 页眉-页脚设计  %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%\renewcommand{\headrulewidth}{3pt} %页眉(单)线宽(默认黑色)，设为0可以去页眉线
%\makeatletter % 双线页眉
%\def\headrule{\color{blue}{\if@fancyplain\let\headrulewidth\plainheadrulewidth\fi%
%\hrule\@height 0.5pt \@width\headwidth\vskip1pt %上面线为0.5pt粗
%\hrule\@height 3.0pt\@width\headwidth  %下面3pt粗
%\vskip-2\headrulewidth\vskip-1pt}      %两条线的距离1pt
%  \vspace{6mm}}     %双线与下面正文之间的垂直间距
%\makeatother

%%\renewcommand{\footrulewidth}{3pt} %页脚线宽(默认黑色)，设为0可以去页脚线
%\makeatletter % 双线页眉
%\def\footrule{{\color{blue}{\if@fancyplain\let\footrulewidth\plainfootrulewidth\fi%
%\hrule\@height 3.0pt \@width\headwidth}}
%  \vspace{2mm}}
%\makeatother

%\pagestyle{fancy}    %与文献引用超链接style有冲突
%\lhead{\bfseries Result} %页眉左边位置内容，并加粗 
%\chead{} % 页眉中间位置内容
%\rhead{\includegraphics[scale=0.20]{Figures/BCC_logo-1.png}}%在此处插入logo.pdf图片 图片靠右
%\lfoot{}  %页脚
%\rule{\temptablewidth}{1pt}
%\cfoot{}
%\rfoot{}
\fancyfoot[C]{} %去掉页码
%%%%%%%%%%%%%%%%%  % pagestyleR常用格式  %%%%%%%%%%%%%%%%%%%%%%%%%
%% empty 无页眉页脚
%% plain 无页眉，页脚为居中页码
%% headings 页眉为章节标题，无页脚
%% myheadings 页眉内容可自定义，无页脚
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{CJK}{UTF8}{gbsn} %针对文字编码为unix %CJK自带的utf-8简体字体有gbsn(宋体)和gkai(楷体)
%\begin{CJK}{GBK}{hei}	%针对文字编码为doc
%\begin{CJK}{GBK}{hei}	 %针对文字编码为doc
%\CJKindent     %在CJK环境中,中文段落起始缩进2个中文字符
%\indent
%
\renewcommand{\abstractname}{\small{\CJKfamily{hei} 摘\quad 要}} %\CJKfamily{hei} 设置中文字体，字号用\big \small来设
\renewcommand{\refname}{\centering\CJKfamily{hei} 参考文献}
%\renewcommand{\figurename}{\CJKfamily{hei} 图.}
\renewcommand{\figurename}{{\bf Fig}.}
%\renewcommand{\tablename}{\CJKfamily{hei} 表.}
\renewcommand{\tablename}{{\bf Tab}.}
%\renewcommand{\thesubfigure}{\roman{subfigure}}  \makeatletter %子图标记罗马字母
%\renewcommand{\thesubfigure}{\tiny(\alph{subfigure})}  \makeatletter　%子图标记英文字母
%\renewcommand{\thesubfigure}{}  \makeatletter　%子图无标记

%将图表的Caption写成 图(表) Num. 格式
\makeatletter
\long\def\@makecaption#1#2{%
  \vskip\abovecaptionskip
  \sbox\@tempboxa{#1. #2}%
  \ifdim \wd\@tempboxa >\hsize
    #1. #2\par
  \else
    \global \@minipagefalse
    \hb@xt@\hsize{\hfil\box\@tempboxa\hfil}%
  \fi
  \vskip\belowcaptionskip}
\makeatother

\newcommand{\keywords}[1]{{\hspace{0pt}\small{\CJKfamily{hei} 关键词:}{\hspace{2ex}{#1}}\bigskip}}

%%%%%%%%%%%%%%%%%%中文字体设置%%%%%%%%%%%%%%%%%%%%%%%%%%%
%默认字体 defalut fonts \TeX 是一种排版工具 \\		%
%{\bfseries 粗体 bold \TeX 是一种排版工具} \\		%
%{\CJKfamily{song}宋体 songti \TeX 是一种排版工具} \\	%
%{\CJKfamily{hei} 黑体 heiti \TeX 是一种排版工具} \\	%
%{\CJKfamily{kai} 楷书 kaishu \TeX 是一种排版工具} \\	%
%{\CJKfamily{fs} 仿宋 fangsong \TeX 是一种排版工具} \\	%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\addcontentsline{toc}{section}{Bibliography}

%-------------------------------The Title of The Paper-----------------------------------------%
%\title{标题}
%----------------------------------------------------------------------------------------------%

%----------------------The Authors and the address of The Paper--------------------------------%
%\author{
%作者:
%\small
%Author1, Author2, Author3\footnote{Communication author's E-mail} \\    %Authors' Names	       %
%\small
%(The Address，City Post code)						%Address	       %
%}
%\affil[$\dagger$]{清华大学~材料加工研究所~A213}
%\affil{清华大学~材料加工研究所~A213}
%\date{}					%if necessary					       %
%----------------------------------------------------------------------------------------------%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\maketitle
%\thispagestyle{fancy}   % 首页插入页眉页脚 

%-------------------------------------------------------------------------------The Abstract and the keywords of The Paper----------------------------------------------------------------------------%
%\begin{abstract}
%The content of the abstract
%\end{abstract}

%\keywords{Keyword1; Keyword2; Keyword3}

%-------------------------------------------------------------------------------The Content of The Paper----------------------------------------------------------------------------------------------%
%\tableofcontents %% 制作目录(目录是根据标题自动生成的)
%-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------%

\newpage	        % 每个新的/newpage 即可有新的\thispagestyle 引领      %
\pagestyle{empty}   % 插入页眉页脚                                        %
%----------------------------------------------------------------------------------------The Body Of The Paper----------------------------------------------------------------------------------------%
%Introduction

\section{Introduction}
进入20世纪90年代，伴随着理论化学、计算物理方法的快速发展以及计算机软硬件技术不断升级和更新，计算材料科学获得了空前发展，它与物理、化学、工程力学以及应用数学等诸多基础和应用学科日益交叉并融合，逐渐成为一门新兴学科，在材料研究中发挥越来越重要的作用\upcite{NatMat3-429_2004,App-CataA254-5_2003,JACS125-4306_2003,JCombChem5-472_2003,Meas_Sci-Tech16-1_2005,Nature392-694_1998}。尤其值得注意的是，近年来，得益于高精度的多尺度计算方法和高性能并行计算技术的突破\upcite{PRL88-255506_2002,Nano-Lett3-1183_2003}，高通量(\textrm{high~throughput})材料计算\upcite{Nature410-643_2001}最先借鉴自药物合成中的组合合成与筛选\textrm{(combinatorial synthesis and screening)}的思想而出现的，组合与筛选研究兴起于1990年代中期\upcite{Science268-1738_1995}，在21世纪最初十年，逐渐扩展到计算材料研究领域，形成“高通量材料计算”的理念。在创新发展新材料、发现新现象方面显现出强大的能力，借助机器学习技术进行材料性能预测，加速材料属性改善、优化和提升，是近年来的研究热点，前景广阔。

材料数据库是支持新材料研发和加速材料性能提升的重要依据，早在计算材料发展之前，人类已经通过实验手段积累了大量的材料数据。但是限于实验条件的限制，很多数据库的材料物性数据并不完整。这些数据也是产生和检验第一原理模拟结果的重要来源和依据。而随着“材料基因组计划”的实施，为计算材料科学提供了越来越广阔的应用空间，高性能计算与\textrm{DFT}相结合推动了第一原理材料模拟数据生成和结果分析的自动化，涌现了越来越多的计算材料数据库，特别是在功能材料领域，高通量\textrm{DFT}计算大大推进了新材料合成的进步。\upcite{JCED59-3232_2014, IC53-11849_2014,JPCL4-3607_2013,PCCP16-22073_2014}

%\chapter{高通量第一原理计算数据挖掘}\label{chap:datamining} 
在传统的研究范式下，获取材料物性数据的成本，无论是通过实验手段还是计算模拟，代价都是比较高的，虽然高通量第一原理计算自动流程和数据库解决了材料物性数据的获取问题，但是并未给出现有材料数据基础上的物性优化的方案，因此利用数据挖掘技术，实现数据驱动的材料物性筛选、预测和提升的技术路线，有着特殊重要的意义。机器学习\textrm{(Machine Learning, ML)}技术可以从大量数据中获得有价值的信息，尤其是面对高维复杂数据时，机器学习技术是确定数据间关系的有力的工具。

机器学习是自动完成数据分析并提取数据关系的一类方法的统称，获取的数据关系可用于预测未知数据或辅助不确定条件下的决策过程\upcite{ML_2012}。传统定义界定的机器学习，是指无须借助解析程序，直接依靠数据来提升任务处理的性能\upcite{IBMJRD3-211_1959}，自从1950年代统计学、计算科学与技术和神经科学的发展，机器学习的研究发展到了更广泛的人工智能\textrm{(Artificial Intelligence, AI)}领域。图\ref{AI-ML}表明了人工智能和机器学习的层次关系。
\begin{figure}[h!]
\centering
\vspace*{-0.1in}
\includegraphics[height=1.8in]{Hierarchical_description_AI_ML_DL.png}
\caption{\textrm{人工智能与机器学习和深度学习的层次关系示意图(引自文献\cite{JPM2-032001_2019}).}}%
\label{AI-ML}
\end{figure}

几十年来，机器学习的算法已经广泛应用于金融、导航控制、语言处理、游戏竞技、计算机可视化和生物信息学等领域。相反地，如果从非严格角度定义，任何计算机模拟人类智能的算法都可以划归为人工智能，并非一定要应用机器学习算法，也包括决策树、知识库、计算机逻辑等算法。近年来，机器学习领域的深度学习\textrm{(Deep Learning, DL)}异军突起，在很多领域都取得了很好的应用\upcite{DL_2016}。深度学习是仿照生物神经网络(意味着输入输出之间允许有多个类似神经的网络层)结构为主要代表的一种示类学习。

\section{机器学习问题的种类}
一般地，机器学习类问题可以表示为:~对于给定的集合$\mathbf{X}$，可以预测或近似得到未知函数$y=f(\mathbf{X})$\upcite{ML-CI}。集合$\mathbf{X}$构成特征空间，集合中的每个元素$\mathbf{x}$称为特征向量(在材料类的机器学习中也称描述符)。根据机器学习得到的近似函数$\hat{y}=\hat{f}(\mathbf{X})$，模型有能力预测训练数据之外的输出值，机器学习的这种预测能力也称为模型的“泛化”\textrm{(generalization)}。由于机器学习的一般特征，对其的分类，很少有根据输入或输出来划分，主要根据学习的特征分为无监督学习\textrm{(unsupervised learning)}和监督学习\textrm{(supervised learning)}。

无监督学习是描述性质的，就是所有数据只有特征向量没有标签，即$\mathbf{x}_{\mathrm{i}}\in\mathbf{X}$，但是这些数据呈现出聚群的结构，每个相似的类型或特征的会聚集在一起。如果没有标签的数据的组合$f(\mathbf{X})$是有限个，则称为聚类\textrm{(clustering)}，学习的是数据特征的分类;~反之，如果$f(\mathbf{X})\in[0,\infty)$，则称为密度估计\textrm{(density estimation)}，学习的是数据特征的边缘分布。另一种无监督学习是降维\textrm{(dimensionality reduction)}，是对数据实施压缩，用少量输入变量代表数据，特别是当$f(\mathbf{X})$是高维形式，降维后可以更清晰地了解复杂数数据的检测模式。

与无监督学习相对反，监督学习是预测性质的，是通过学习指定数量的输入输出间的函数映射$(x_{\mathrm{i}},y_{\mathrm{i}})\in(\mathbf{X},f(\mathbf{X}))$(取$i=1,\cdots,N$，称为训练集\textrm{(training set)})，如果输出函数$y_{\mathrm{i}}$表示类别的有限集合，则称为分类\textrm{classification}问题，该模型可用来预测未知数据所属类型;~如果输出函数是实数，即$y_\mathrm{i}\in\mathbb{R}$，则称为回归\textrm{(regression)}问题，模型用来预测未知输入数据对应的值输出值。此外的机器学习问题还包括:~
\begin{itemize}
	\item 半监督学习，即大部分没有映射关系的数据和少量有映射关系的数据;
	\item 多任务和迁移学习，即将从相关问题习得的知识应用到数据极少的对象，提升模型的学习能力;
	\item 强化学习，即没有输入输出，但会和环境不断交互，通过最大化环境的反馈，最终达到学习目标。
\end{itemize}

机器学习的工作流程可以概括为\upcite{Efficient_ML}:
\begin{itemize}
	\item 数据的收集和筛选:~从现有数据中产生并选择与问题解决有用和相关的数据子集。
	\item 数据预处理:~将数据以合适的形式表示出来，清洗缺失和不完整数据，将数据转换成统一的形式(如整型、字符串型等等)。必要时还须根据需要将数据按问题的格式重新表示。
	\item 应用机器学习算法训练数据:~将数据分为训练集、验证集和测试集三部分，训练集数据用于学习并得到模拟参数(主要针对监督学习)。
	\item 模型测试和优化:~用验证集数据评估模型的效果和性能，并用验证集数据优化模型;~一旦完成优化，用测试集数据评定模型的性能。如果学习不成功，选用改善的数据重复上述步骤或改变学习算法。
	\item 应用:~将得到的有效模型对未知数据进行预测，只要有新的数据，模型还可以继续训练。
\end{itemize}

\section{机器学习算法简介}
根据文献\cite{IEEE-TEC1-67_1997,NC8-1341_1996}的讨论，在机器学习中尚未有放之四海而皆准的方案，因此必须面向问题选择合适的机器学习算法。对于一个具体的问题，选择合适的机器学习算法至关重要，可选的算法很多，但每种学习方法有其侧重的问题和数据集。数据集可分为两类，有标签的和没有标签的。对于有标签的，任务就是通过监督学习算法，建立数据和标签的映射关系$\{\mathbf{x}^{(\mathrm{i})}\}\rightarrow\{y^{(\mathrm{i})}\}$;~而对于无标签数据，则是用无监督学习方法确定数据集的结构。

对于海量数据，很可能存在非常多的特征向量(也称为“维度灾难”\textrm{(the curse of dimensionality)})。在机器学习算法中，一般通过主成分分析(\textrm{principal component analysis, PCA})来实现数据维度压缩的主要方法\upcite{OJS6-701_2016}。将高维空间中将坐标轴旋转到数据点集中的区域，并使数据沿轴向变化最大，由此可以实现数据的压缩。

对于无监督学习来说，除了主成分分析之外，数据集分类用得最多的算法是\textit{k}-\textrm{means}算法\upcite{EJHPS4-2_2008}。\textit{k}-\textrm{means}算法通过直接计算数据点和聚类中心的欧氏距离\textrm{(Euclidean distance)}，将$n$个数据分类为$k$个子集($k<n$)。\textit{k}-\textrm{means}聚类算法的结果与初始聚类中心位置的选择密切关联，如果初始聚类中心选择得不同，结果差别会比较大。一般克服该问题的策略是通过多次初始聚类中心并执行该算法，最终选择最有代表性的聚类形式作为结果。

层次聚类(\textrm{Hierarchical Clustering})算法也是一种常见的非监督学习方法，层次聚类通过一层一层的进行聚类，既可以由上向下把大的类别分割;~也可以由下向上对小的类别进行聚合。

对监督学习来说，每个数据不仅有特征向量$\mathbf{x}_{\mathrm{i}}$，还会有相应的标注$y_{\mathrm{i}}$。如果标注量允许连续变化，最常选用的算法是线性回归\textrm{(Linear Regression)}。回归算法的基本思想是，对于满足正态分布的数据点，可有参数拟合的预测表达式
\begin{equation}
	\hat y^{(\mathrm{i})}=\theta^T\mathbf{x}^{(\mathrm{i})}
	\label{eq:linear_eq}
\end{equation}
上标$T$表示矢量的转置，$\hat y^{(\mathrm{i})}$是预测的标注值，$\theta$是参数的矢量。为了求得$\theta$参数，定义误差的最小二乘函数
\begin{equation}
	J(\theta)=\sum_{i=1}^nL[\hat{y}^{(\mathrm{i})}(\mathbf{x}^{(\mathrm{i})},\theta),y^{(\mathrm{i})}]=\dfrac12\sum_{i=1}^n(\theta^T\mathbf{x}^{(\mathrm{i})}-y^{(\mathrm{i})})^2
	\label{eq:linear_2}
\end{equation}
最小化该函数，可以得到最优化的参数$\theta$，由此可以得到线性回归机器学习的模型。不难看出，最优化参数$\theta$用矩阵表示可写作:~
\begin{displaymath}
	\theta=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
\end{displaymath}
这里$\mathbf{X}$矩阵的每一列是由训练集输入数据$\mathbf{x}^{(\mathrm{i})}$，$\mathbf{y}$是对应的输出标注构成的矢量。

一旦训练出机器学习的模型，模型的性能可以通过测试数据集检验。根据训练集数据的多寡，预测性能有两种可能
\begin{itemize}
	\item 训练集数据不够，未能完全反应模型特征，预测结果将会表现出明显的偏差
	\item 训练集数据过多，模型可以很好地体现训练集特征，但是对训练集外的数据效果不好，也就是出现过拟合\textrm{(overfitting)}现象。
\end{itemize}
\begin{figure}[h!]
\centering
\vspace*{-0.1in}
\includegraphics[height=3.2in]{The_optimum_comlexity_vs_prediction.png}
\caption{\textrm{线性回归中测试集数据多寡引起的预测误差机器优化示意图(引自文献\cite{Brunet_Thesis_2010}).}}%
\label{ML_Fitting_Error}
\end{figure}
图\ref{ML_Fitting_Error}给出两种不同上述两种极端情况的预测误差示意，不难看出，误差与训练数据集包含的数据数量密切相关。实际应用中，常常引入标准化参数$\lambda$来反应训练集元素变化的影响，优化参数$\lambda$以降低训练集数据数量的影响。这种实际上是对线性回归的发展，常见的有岭回归\textrm{(Ridge Regression)}和套索回归\textrm{(LASSO Regression)}。引入$\lambda$作为正则化参数，得到函数
\begin{equation}
	J(\theta)=\dfrac12\sum_{i=1}^n(\theta^T\mathbf{x}^{(\mathrm{i})}-y^{(\mathrm{i})})^2+\lambda\|\theta\|_p
	\label{eq:Ridge_LASSO}
\end{equation}
这里$p$表示数据度量形式。$p=1$时是\textrm{LASSO}回归，函数最小化要求将会排除某些训练集数据特征;~$p=2$是岭回归，函数最小化时会排除训练集中特征贡献大的部分。因此无论是岭回归还是\textrm{LASSO}回归，含参数$\lambda$的正则项将通过压制或筛选训练数据集中的特征来调节其对误差函数的贡献。需要注意的是$\lambda$不能像$\theta$一样优化，一般是通过比较几个不同的值，选其中能最大化预测能力又不会引入太大偏差的一个值。

监督学习中另一类称为分类算法，广泛应用于数据集的标注为离散标签场景。分类算法中比较流行的是逻辑回归\textrm{(logistic regression)}，可以类比为映射到区间[0,1]之间的线性回归:~比如对于给定数据点$\mathbf{x}^{\mathrm{i}}$，将其分入特定的“是”类($y^{\mathrm{(i)}}=1$)或“否”类($y^{\mathrm{(i)}}=0$)，因此预测函数可以表示为
\begin{equation}
	\hat y=\sigma(\theta^T\mathbf{x})=\dfrac1{1+\mathrm{e}^{-\theta^T\mathbf{x}}}
	\label{eq:logstical_y}
\end{equation}
这里$\theta$仍是参数矢量，$\sigma$是逻辑函数(或称\textrm{sigmoid}函数)。一般来说，如果数据点$\mathbf{x}^{(\mathrm{i})}$对应的计算标签值$\hat{y}^{(\mathrm{i})}\leqslant0.5$，就应该考虑标注为$y^{(\mathrm{i})}$，所以预测函数也可以视为条件概率:$\hat{y}=P(y=1|\mathbf{x},\theta)$。

对于分类问题，误差函数可以定义为负的$\log$-型函数(也称为交互熵\textrm{cross-entropy})，同样要求参数$\theta$能够最小化该函数:
\begin{equation}
	J(\theta) = -\dfrac1n\sum_{i=1}^n[y^{(\mathrm{i})}\log(\hat{y}^{(\mathrm{i})})+(1-y^{(\mathrm{i})})\log(1-\hat{y}^{(\mathrm{i})})]
	\label{eq:logstical_eq}
\end{equation}
这里$y^{(\mathrm{i})}$和$\hat{y}^{(\mathrm{i})}=\sigma(\theta^T\mathbf{x}^{(\mathrm{i})})$分别为实际和预测的二元标签值。和线性回归类似，上式\eqref{eq:logstical_eq}中也可以引入正则参数$\lambda$。逻辑分类也可用于处理超过两类的数据分类，这种情形下，需要训练有$n$个逻辑回归值的模型，每一类对应一个数值，每个数据分入概率值最高的类中。

\textrm{Corsts}和\textrm{Vapnik}在逻辑回归的一系列改造基础上，提出了最通用的分类算法:~支持向量机\textrm{(Support Vector Machines, SVMs)}\upcite{ML20-273_1995}。该算法定义分类判别函数
\begin{equation}
	J(\theta)=C\sum_{i=1}^n[y^{(\mathrm{i})}\max(\theta^T\mathbf{x}^{(\mathrm{i})},0)+(1-y^{(\mathrm{i})})\max(-\theta^T\mathbf{x}^{(\mathrm{i})},0)]+\dfrac1n\sum_{i=1}^n\theta_i^2
	\label{eq:SVM_cost}
\end{equation}
这里$C$是参数。式\eqref{eq:SVM_cost}可以理解为在约束条件$y^{(\mathrm{i})}(\theta^T\mathbf{x}^{(\mathrm{i})}+b)\leqslant1$下对全部训练数据点$(\mathbf{x}^{(\mathrm{i})},y^{(\mathrm{i})})$实现$\|\theta\|^2$，再由标签$y^{(\mathrm{i})}$值为$+1$或$-1$确定测试数据$i$是否属于某一特定的类。不难看出，式\eqref{eq:SVM_cost}是对$\|\theta\|^2$引入\textrm{Lagrangian}乘子并最小化得到的。

\textrm{SVM}的最重要的特色是内核技巧\textrm{(kernel trick)}，将参数矢量$\theta$用训练集数据表示
\begin{displaymath}
	\theta=\sum_i\alpha_iy^{(\mathrm{i})}\mathbf{x}^{(\mathrm{i})}
\end{displaymath}
因此可以将分类规则写成数据点积的形式
\begin{equation}
	\theta^T\mathbf{x}+b=\sum_i\alpha_iy^{(\mathrm{i})}\mathbf{x}^{(\mathrm{i})}\cdot\mathbf{x}+b\leqslant0\rightarrow y=+1
	\label{eq:SVM_cp}
\end{equation}
这里$b$和$\{\alpha_i\}$是待学习的参数，内核技巧利用映射关系$\phi(\mathbf{x}):~\mathtt{R}^n\rightarrow\mathtt{R}^m$将矢量变换成$\mathbf{x}^{(\mathrm{i})}\cdot\mathbf{x}$点积，实际上这是将数据点映射到更高维度的空间中。所有矢量点积映射的变换都可以类似处理，其中最常用的内核为多项式内核:
\begin{displaymath}
	K(\mathbf{x}^{(\mathrm{i})},\mathbf{x}^{(\mathrm{j})})=\phi(\mathbf{x}^{(\mathrm{i})})\cdot\phi(\mathbf{x}^{(\mathrm{j})})=(\mathbf{x}^{(\mathrm{i})}\cdot\mathbf{x}^{(\mathrm{j})}+1)^d,~d\in\mathtt{N}
\end{displaymath}
和\textrm{Gaussian}内核(也叫径向基函数(\textrm{radial basis function, RBF})内核)
\begin{displaymath}
	K(\mathbf{x}^{(\mathrm{i})},\mathbf{x}^{(\mathrm{j})})=\mathrm{e}^{\dfrac{-\|\mathbf{x}^{(\mathrm{i})},\mathbf{x}^{(\mathrm{j})}\|^2}{2\sigma^2}}
\end{displaymath}
这里$\sigma$是可调参数，\textrm{Gaussian}内核衡量数据点在高维空间里的相似度，常常用于模式匹配。
%这里因为引入极大值函数$\max(z,0)$，可在数据空间将分类

到目前为止讨论的分类算法都是基于判别模式\textrm{(discriminative model)}的，即对于数据点预测的标签是条件概率$p(y|\mathbf{x})$，还有一些分类算法是采用另一种思路来达到同样的目的，即基于生成模式\textrm{(generative model)}，就是将待定的条件概率$p(\mathbf{x}|y)$用后验\textrm{Bayes}公式表示:
\begin{equation}
	p(y|\mathbf{x})=\dfrac{p(\mathbf{x}|y)p(y)}{p(\mathbf{x})}=\dfrac{p(\mathbf{x}|y)p(y)}{\sum\limits_ip(\mathbf{x}|y=i)p(y=i)}
	\label{eq:Bayes}
\end{equation}
这里$p(y)$表示先验概率，即没有附加任何先期知识和分析得到的概率。假设数据点的特征向量$\mathbf{x}^{(\mathrm{i}})$和标注$y^{(\mathrm{i})}$之间完全独立，可有\textrm{Na\"ive~Bayes}分类算法\upcite{Naive-Bayes}，改写了式\eqref{eq:Bayes}的后验概率形式:
\begin{equation}
	p(y|\mathbf{x})=\dfrac{\prod\limits_{j=1}^np(x_j|y)p(y)}{p(\mathbf{x})}
	\label{eq:Naive-Bayes}
\end{equation}
$x_j$表示特征向量$\mathbf{x}$的元素。训练分类前先列出训练数据点的全部先验概率$p(y)$和条件概率$p(x_j|y)$，然后用\textrm{Na\"ive~Bayes}算法计算，并选择所有$y$中最大的后验概率$p(y|\mathbf{x})$作为分类预测值。

还有一种类似的简单分类算法是\textit{k}-最近邻(\textit{k}-\textrm{nearest neighbors, kNN})算法，该算法利用数据点空间距离的类似性，无须对数据再进行训练，因此对于处理快速任务特别具有吸引力。简言之，如果$d$-维空间有训练集数据$\{\mathbf{x}^{(\mathrm{i})}\}$，\textrm{kNN}计算未知数据点与这些数据点之间的空间距离
\begin{displaymath}
	d(\mathbf{x},\mathbf{x}^{(\mathrm{i})})=\|\mathbf{x}-\mathbf{x}^{(\mathrm{i})}\|_p
\end{displaymath}
这里的$p$是维度参数。一旦得到$\mathbf{x}$到空间各点的距离，$\mathbf{x}$点归入与其有最近邻$k$值最大的类中，如果没有最大类，则随机归入最近邻点的最常使用的标注类中。显然，对连续的标签值求平均，就是基于\textrm{kNN}的回归。类似地，如果$k$值的选取对于分类很敏感，不同的$k$值很可能得到完全不同的数据分类。

也有些机器学习算法同时适合分类和回归，比如决策树\textrm{(Decision Trees)}就是这类通用并快捷的机器学习算法。因为决策树算法应用场景非常广，限于篇幅，这里只介绍两种最通用的两种决策树，分别是分类树\textrm{CART}算法\upcite{ML_CART}和回归决策树\textrm{C4.5}算法\upcite{ML_C4.5}，这两种算法都是基于数据空间的划分实现的，空间划分时可通过创建节点来实现对某种分裂算法的优化，决策树上的每个节点都包含一个定义该部分空间划分的问题，直到空间不可再分，每个不连通的子空间称为叶节点，叶节点包含了待分类或预测的数据点。

\textrm{C4.5}对训练集\textrm{S}完成一系列的空间多重划分操作，划分的目标划分能使得测试集\textrm{B}信息收益$G(S,B)$与潜在信息$P(S,B)$比最大化，即
\begin{displaymath}
	\mathrm{argmax}_B\dfrac{G(S,B)}{P(S,B)}
\end{displaymath}
这里信息收益$G(S,B)$定义为
\begin{displaymath}
	G(S,B)=-\sum_{i=1}^kf_i\log(f_i)+\sum_{j=1}^l\dfrac{|S_j|}{|S|}\sum_{i=1}^kf_i^{(\mathrm{i})}\log(f_i^{(\mathrm{j})})
\end{displaymath}
其中$f_i$是测试集\textrm{S}中元素属于类$C_i$的相关频率，而$f_i^{(\mathrm{i})}$是与测试集合划分空间$S_j$对应的测试集合$B$的同相关频率。

潜在信息的定义为
\begin{displaymath}
	P(S,B)=-\sum_{i=1}^l\dfrac{|S_i|}{|S|}\log\bigg(\dfrac{|S_i|}{|S|}\bigg)
\end{displaymath}
直到节点中只包含一类数据或者无法区分特征的数据，则停止空间的划分。

相反\textrm{CART}是每次只执行二重划分的决策树方法，在这种分类策略下，其标准是确保\textrm{Gini}指数(不纯度系数)最小化。\textrm{Gini}指数的定义为
\begin{displaymath}
	I_G(S)=1-\sum_{j=1}^kf_j^2
\end{displaymath}
$S$表示训练集，$f_i$是训练集中第$j$类的相关频率。采用\textrm{CART}算法做回归学习，要注意有两个困难:
\begin{itemize}
	\item 节点预测的是实际数目而并非类
	\item \textrm{CART}中的分裂算法的标准是使得重置估计\textrm{(resubstitution estimate)}最小，重置估计的定义为方差
		\begin{displaymath}
			R(S)=\dfrac1n\sum_{i=1}^n[y_i-\hat{y}_i]^2
		\end{displaymath}
		这里$y_i$是第$i$个数据的标签，而$\hat{y}_i$则是对应的估计值。
\end{itemize}
所以采用这种划分的结果，是每个分区的预测值都是该分区中的平均值。换句话说，对于回归学习，\textrm{CART}输出函数的是分段常数。

回归决策树的一个主要问题是一旦开始训练，往往伴随过拟合。为了克服决策树的过拟合，一种办法是修剪决策树分叉，这样做会损失一定的精度，但是提高了回归的泛化能力;~更好的方法是采用随机森林\textrm{(Random Forsets)}，随机森林实际上一种系综平均方法，即训练大量的决策树然后再取统计平均值\upcite{ML45-5_2001}。也就是说，在随机森林方法中，决策树将成为训练对象。对决策树的特征随机抽样训练时，一般采取自展抽样\textrm{(bootstrap sample)}方案。随机森林算法将一系列的非强化学习的功能组合起来，使其对未知数据有更好的预测能力。

人工神经网络\textrm{(Artifical Neural Networks, ANNs)}是模仿人脑神经元结构处理信息的一类机器学习算法。神经网络的基本思想可以用图\ref{ML_ANN}表示，每个网络层由若干神经元组成，网络层内和相邻网络层内神经元间彼此联结构成完整的神经网络。神经网络可用于分类或回归，主要网络架构包括前馈神经网络\textrm{(feed-forward NNs)}、循环神经网络\textrm{(recurrent NNs)}和卷积神经网络\textrm{(convolutional NNs)}等，不同架构的主要区别在于神经元之间的联结方式以及神经元对数据处理方式不同。
\begin{figure}[h!]
\centering
\vspace*{-0.1in}
\includegraphics[height=2.5in]{ANN_Algorithm.png}
\caption{含有多个隐藏层的人工神经网络的基本示意图.}%
\label{ML_ANN}
\end{figure}

对于一个人工神经网络来说，输入层\textrm{(input layer)}用于接收来自训练集的描述符向量(\textrm{descriptor vector})，经过隐藏层\textrm{(hidden layer)}中一系列的非线性操作(数据在各层间依次传输)，最后在输出层\textrm{(output layer)}输出。数据结果可以是分类，也可以是回归的值。

根据神经网络的算法规则，第$k$层的第$i$个神经元输入$z_i^{(\mathrm{k})}$来自前一层的输出$y_j^{(\mathrm{k-1})}$
\begin{displaymath}
	z_i^{(\mathrm{k})}=\omega_{i0}^{(\mathrm{k})}+\sum_jy_j^{(\mathrm{k-1})}\omega_{ij}^{(\mathrm{k})}
\end{displaymath}
这里的$\omega_{ij}^{(\mathrm{k})}$是关联临近层的矩阵元，这里$\omega_{i0}^{(\mathrm{k})}$称为偏移量\textrm{(bias)}。数据在神经元处经过非线性变换(或称活化)，常见的函数形式有
\begin{itemize}
	\item \textrm{sigmoid}函数
\begin{displaymath}
	y_i^{(\mathrm{k})}=\dfrac1{1+\mathrm{e}^{-z_i^{(\mathrm{k})}}}
\end{displaymath}
\item 双曲正切\textrm{(hyperbolic tangent)}函数
\begin{displaymath}
	y_i^{(\mathrm{k})}=\dfrac{\mathrm{e}^{z_i^{(\mathrm{k})}}-\mathrm{e}^{-z_i^{(\mathrm{k})}}}{\mathrm{e}^{z_i^{(\mathrm{k})}}+\mathrm{e}^{-z_i^{(\mathrm{k})}}}
\end{displaymath}
\item \textrm{ReLU(rectifying linear function)}函数
	\begin{displaymath}
		y_i^{(\mathrm{k})}=\left\{
			\begin{aligned}
			& z_i^{(\mathrm{k})},~z_i^{(\mathrm{k})}>0\\
& 0,~z_i^{(\mathrm{k})}\leqslant0 
			\end{aligned}
\right.
	\end{displaymath}
\end{itemize}
不难看出，在神经网络中，因为前一层输出的矢量，经过当前层神经元的处理，就映射到了新的矢量空间，因此神经网络适合于处理分类问题，特别是进行复杂决策。

神经网络用于回归问题，评估预测精度的方法就是计算方差。比如对于二元分类问题，每个神经元面向输入相应分类的分类概率，采用单值\textrm{sigmoid}函数计算输出值，这种预测精度和逻辑回归算法的精度相同，表示误差的交互熵也相同。两种方法的区别在于逻辑回归学习的参数(矢量$\theta$)变成层间矩阵($\omega_{ij}^{(\mathrm{k})}$)，预测的结果也将由更复杂的非线性函数计算得到的。对于多元分类，神经元面向输入数据，采用\textrm{softmax}函数\footnote{\textrm{softmax}函数又称归一化指数函数，是二元分类函数\textrm{sigmoid}在多元分类上的推广，目的是将多分类的结果以概率形式表示出来。}，因此用$\mathbf{y}^{(\mathrm{k}-1)}=[y_1^{(\mathrm{k}-1)},y_2^{(\mathrm{k}-1)},\cdots,y_n^{(\mathrm{k}-1)}]$表示的类$y_i$的概率为
\begin{displaymath}
	y_i^{(\mathrm{k})}=\dfrac{\mathrm{e}^{y_i^{(\mathrm{k}-1)}}}{\sum\limits_{j=1}^n\mathrm{e}^{y_i^{(\mathrm{k}-1)}}}
\end{displaymath}
待最小化的损失函数(即损失函数或称交互熵)为
\begin{displaymath}
	L(\{\omega^{(\mathrm{k})}\})=-\sum_{ijk}y_{ij}\log[\hat{y}_{ij}(\{\omega^{(\mathrm{k})}\})]
\end{displaymath}
此处$\{\omega^{(\mathrm{k})}\}$是待学习的一套神经网络权重矩阵，$y_{ij}$是第$j$层训练的第$i$个输出标签，$\hat{y}_{ij}$则是与$y_{ij}$的预测值。计算损失函数$L$的梯度，并通过梯度下降法获得损失函数极小值，即可确定优化的参数$\{\omega_{ij}^{(\mathrm{k})}\}$，这一过程称为反向传播\textrm{(back-propagation)}。

一言以蔽之，机器学习的人工神经网络预测主要由如下流程实现:
\begin{itemize}
	\item 随机生成一套的初始权重矩阵$\{\omega_{ij}^{(\mathrm{k})}\}$;
	\item 将训练集数据提交到神经网络计算，并得到输出结果;
	\item 通过损失函数计算预测结果和训练集的偏差;
	\item 由反向传播得到损失函数的权重;
	\item 最小化损失函数得到优化的权重矩阵。
\end{itemize}
如果对训练集中全部样本数据都完成上述流程，则称为在线学习\textrm{(online learning)}，如果只是对一部分数据执行上述全部流程，则称为中等批量学习\textrm{(mid-batch learning)}或简称批量学习\textrm{(batch learning)}。

对于神经网络的优化，除了反向传播-梯度下降，还有一种通用的优化策略是遗传算法\textrm{(genetic algorithm, GA)}，适用于求解缺乏好的数学特征的目标函数(例如没有明确导数的函数)的神经网络的权重。遗传算法是通过模拟\textrm{C.~Darwin}的“自然进化”(\textrm{nature evolution})思想，搜索并获得最优解的:~
	\begin{itemize}
		\item 优化问题可能潜在的解集构成一个种群(\textrm{population})，该种群由经过基因(\textrm{gene})编码的一定数目的个体(\textrm{individual})组成，每个个体是染色体(\textrm{chromosome})带有特征的实体;
%		\item 优化问题可能潜在的解集构成一个种群，该种群由经过基因编码的一定数目的个体组成，每个个体是带有一定的优化特征
%		\item 种群产生之后，借助于自然遗传学的遗传算子(\textrm{genetic operators})进行组合交叉(\textrm{crossover})和变异(\textrm{mutation})，产生新一代个体
		\item 种群产生之后，借助于自然遗传学的遗传算子进行组合交叉(\textrm{crossover})和变异(\textrm{mutation})，产生新一代个体，如图\ref{Optimize_GA}所示;
\begin{figure}[h!]
\vspace*{-0.10in}
\centering
\includegraphics[height=1.5in]{Genetic_Algorithm_basic.png}
\caption{遗传算法中通过交叉和变异获得新一代个体的示意图.}%
\label{Optimize_GA}
\end{figure}
		\item 按照适者生存和优胜劣汰的原理，在每一代，根据问题域中个体的适应度(\textrm{fitness})，选择(\textrm{selection})合适的个体，构成代表新的解集的种群;
%		\item 按照适者生存和优胜劣汰的原理，在每一代，根据问题域中个体的适应度，选择合适的个体，构成代表新的解集的种群
		\item 逐代演化(\textrm{generation evlution})后，获得优化结果(逼近优化目标)。
%		\item 逐代演化后，产生出越来越好的近似解(优化目标)
	\end{itemize}

对于监督学习算法来说，通过对训练集样本数据计算损失函数(方差形式或交互熵形式)的最小化，得到相应的优化参数，训练过程即告结束。但是这种模型是否仅对训练数据有效，还需要通过另外的一些数据集来检验，这一过程称为验证，对应的数据集称为验证集。因此监督学习的数据集一般分为三类:~训练集、验证集和测试集，这些样本数据最好是具备相同的统计分布特征。为了优化学习模型，建议先对样本数据多次学习，最后才将模型应用到测试数据集上，对比预测数据和实际数据的偏差，可以评估模型的真实预测能力。图\ref{ML_Fitting_Error}给出了机器学习优化过程中数据与误差的平衡示意，不难想象，当数据非常有限时，如果再从训练样本中分出一部分留作测试数据集，难免捉襟见肘，影响训练模型的效果，必须另想对策。面向这样的应用场景，最常用的方案是$k$-重交叉验证\textrm{cross-validation}:~首先将训练集分成$k$个子集，选择$k-1$个子集训练模型，并用剩下的一个未训练的子集验证模型。将训练-验证过程执行$K$次，并将$K$次验证的平均损失函数来评估模型性能的平均表现。平均损失函数定义为:
\begin{displaymath}
	E_{\mathrm{cv}}^{K}=\dfrac1K\sum_{k=1}^K\sum_{i=1}^{n_k}L(\hat{y}_k^{(\mathrm{i})},y^{(\mathrm{i})})
\end{displaymath}
这里$L$是验证数据集的损失函数，$\hat{y}_k^{(\mathrm{i})}$是用训练子集(不含验证子集$k$)得到的模型，对采样数据$i$预测的标签值$\hat{y}_k^{(\mathrm{i})}$，训练子集的采样数据共有$n_k$。特别地，$K=n$，即训练集划分的子集与其元素个数相同，则称为差一交叉验证\textrm{(leave-one-out cross-validation)}。


交叉验证也可用于评估训练模型中超参数的性能，超参数包括正则化参数$\lambda$、\textrm{SVM}的\textrm{Gaussian}内核参数$\sigma$等。还有一些参数隐藏得更深，比如二叉树(二元分类决策树)的修剪层次和构成随机森林的系综包括的特征向量等，这些参数也可以用通过交叉验证优化。优化参数就是选择有最小的预测误差时所对应的参数。

对机器学习模型性能的评估手段有很多，以二元或多元分类为例，可以选用混同矩阵\textrm{(confusion matrices)}，所谓混同矩阵，就是评估模型预测与抽样吻合程度建立的矩阵，预测吻合度高的元素主要出现在矩阵对角元，预测吻合度低的都在矩阵非对角元。比如以矩阵的列方向为采用数据的标注，行方向是预测数据的标注，如图\ref{ML_CM_ROC}所示。除了用近似单位矩阵表示，也可以用真阳\textrm{(True Positive, TP)}、真阴\textrm{(True Negative, TN)}和假阳\textrm{(False Positive, FP)}假阴\textrm{(False Negative, FN)}填充矩阵。相应地，绘制模型接受行为特征\textrm{(receiver operating characteristic, ROC)}曲线时，也可以用比如“真阳率”\textrm{(TP rate)}~$\mathrm{TPR}=\dfrac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}}$和“假阳率”\textrm{(FP rate)}~$\mathrm{FPR}=\dfrac{\mathrm{FP}}{\mathrm{FP}+\mathrm{TN}}$，不同阈值下的\textrm{ROC}曲线反应了模型的预测能力。
\begin{figure}[h!]
\centering
\vspace*{-0.1in}
\includegraphics[height=1.7in]{ML_confusion_matrix.png}
\hskip 5pt
\includegraphics[height=1.8in]{ML_ROC.png}
\caption{评估机器学习模型的混同矩阵和\textrm{ROC}示意，理想的混同矩阵是单元阵，\textrm{ROC}以下的面积越大，表示模型的性能越好.}%
\label{ML_CM_ROC}
\end{figure}

相应地，回归预测也有很多方案可以评估模型对数据的拟合精度。比如:
\begin{itemize}
	\item 平均绝对误差\textrm{(mean absolute error, MAE)}
		\begin{displaymath}
			\mathrm{MAE}=\dfrac1n\sum_i^n|y_i-\hat{y}_i|
		\end{displaymath}
	\item 归一化相对百分误差\textrm{(normalized mean absolute in percentage, MAPE)}
		\begin{displaymath}
			\mathrm{MAPE}=\dfrac{100\%}n\sum_i^n\dfrac{y_i-\hat{y}_i}{y_i}
		\end{displaymath}
	\item 误差的均方差\textrm{(mean squared error, MSE)}
		\begin{displaymath}
			\mathrm{MSE}=\dfrac1n\sum_i^n(y_i-\hat{y}_i)^2
		\end{displaymath} 
\end{itemize}
从使用频率角度说，由分布参数$\theta$估计$\hat{\theta}^m$与\textrm{MSE}密切关联，即
		\begin{displaymath}
			\mathrm{MSE}=\mathtt{E}[(\hat{\theta}_m-\theta)^2]=\mathrm{Bias}(\hat{\theta}_m)^2-\mathrm{Var}(\theta)
		\end{displaymath} 
		一般更常用的是误差的均方根\textrm{(root of MSE, RMSE)}，此外还有用可决统计系数(也称决定系数%，\textrm{coefficient of determination}
)$\mathrm{R}^2$的，可决统计系数的定义为$\mathrm{R}^2=1-\dfrac{\mathrm{SS}_{res}}{\mathrm{SS}_{tot}}$，这里$\mathrm{SS}_{tot}=\sum\limits_i(y_i-\bar{y})^2$是总的方差求和，而$\mathrm{SS}_{res}=\sum\limits_i(y_i-\hat{y}_i)^2$是预测模型误差平方求和。


\section{数据挖掘与第一原理材料研究}
信息科学与生物学的交叉形成生物信息学极大地带动了信息学科与基础学科的融合，近年来，材料信息学\textrm{(Materials Informatics)}，即应用数据挖掘特别是机器学习技术推动材料科学研究的深入，经过十多年的发展已渐趋成熟\upcite{MT8-38_2005}。材料信息学主要研究材料的内禀特征\textrm{(intrinsic features)}，包括结构、组成、对称性等与材料的性质之间内在的数据关联。通常材料科学研究的习惯思路是已知材料的特征$x_i$，则其影响的材料性质$y_i=f(x_i)$将会如何变化，确定\{\textit{材料}$\rightarrow$\textit{性质}\}的映射关系;~而对于新材料开发领域，往往有逆向式思维，为了获得具有某种性质的材料，则必须使其存在哪些内禀特征。事实上，只有以数据驱动的材料学研究模式，才是能同时回答上述问题的主要形式。

材料科学中数据挖掘特别是机器学习研究的一般思路如图\ref{npjCM}所示，材料学研究的数据挖掘主要是监督学习，即材料物性的预测和材料的分类。对材料物性的预测，机器学习的目标是，应用包括回归在内的各种算法，对目标物性(标签)简历函数关系$f(\mathbf{x})$;~对于分类问题，则根据特定的物性目标，将符合要求的材料归入其中。例如，按照磁性和非磁性划分，和按照晶体所属结构分类就属于两种不同的分类方式，每种分类方式内部各部分之间不能存在交集。根据图\ref{npjCM}，在数据挖掘驱动的材料计算(主要是第一原理材料计算)的主要研究流程为:
\begin{figure}[h!]
\centering
\vspace*{-0.1in}
\includegraphics[height=1.8in]{Materials_informations_workflow.png}
\caption{\textrm{材料科学研究中的数据挖掘方式的一般流程(引自文献\cite{NPJCM3-54_2017}).}}%
\label{npjCM}
\end{figure}
\begin{enumerate}
	\item 问题的确定:~对于机器学习来说，最重要的就是确定待解决的问题的类型，究竟是分类、回归、优化、分布概率估计等中的哪一类。习惯上要根据\textrm{SMART}原则，平衡效率与精度，确定所选的机器学习算法。此外，很重要的是机器学习所需特征向量的选择，尤其需要慎重选择。
	\item 材料数据的组织:~作为机器学习的基本对象，数据是最基本的。对于选定的问题，必须要有充足的数据。即使是最小化的数据集，也应该涵盖研究样本的全部特征(机器学习的输入)和目标物性(机器学习的输出)。对机器学习而言，数据可以是来自理论计算，也可以是实验测量。
	\item 物性的表示:~选用哪些特征向量来表示材料的物性往往决定了机器学习的性能。习惯上，将用于描述材料物性的特征向量称为描述符\upcite{PRL114-105503_2015}(或指纹)。
	\item 机器学习算法和模型的选定、评估与优化:~根据研究问题的目标，选择合适的机器学习算法并对学习结果予以评估。特别要注意算法的精度-效率/性能和训练时长的平衡，同时也要考虑模型的复杂度/合理性。对模型的评估和优化主要针对超参数的选择，既要防止数据不足也要防止过拟合引起的偏差。
\end{enumerate}
机器学习的建模可以简单概述为
\begin{displaymath}
	\mbox{机器学习模型=研究对象+数据+表示+机器学习算法+优化}
\end{displaymath}

\begin{figure}[h!]
\centering
\vspace*{-0.1in}
\includegraphics[height=3.5in]{ML_DFT-1.png}
\caption{\textrm{应用机器学习预测替代高成本量子化学计算的总体思路(引自文献\cite{ML-in-MS_2016}).}}%
\label{ML_QM}
\end{figure}
机器学习除了可作为直接预测材料性质或发现新材料的手段，在第一原理计算方面，更重要的作用是节省或替代获取\textrm{DFT}计算结果或数据所必需的高额成本。在此简要讨论应用机器学习方法来扩展和改进各类模拟计算的主要思路。在类似的计算物理和模拟研究领域，此类机器学习同样有着广泛的应用潜能。对于复杂电子体系，\textrm{Schr\"odinger}方程直接的迭代数值求解对计算资源和时间的消耗都非常高。在\textrm{DFT}框架下，首先通过少量\textrm{DFT}计算获得小规模、高精度的计算结果，对大规模复杂计算对象，应用机器学习加速或避免直接求解\textrm{Kohn-Sham}方程，可以更高效地获得计算结果且不须牺牲精度。机器学习加速\textrm{DFT}计算的基本思路如图\ref{ML_QM}所示。这方面的工作有一定的吸引力，但是注意到模型的复杂性，如何选择合适的特征向量(描述符)始终是加速效果的关键。事实上，机器学习中特征向量的把握，反应了研究者对物理模型理解的深度。从这个角度看，形成良性的人-机交互是机器学习方法取得成功的关键。

通过使用机器学习方法来优化\textrm{DFT}计算里中使用的能量泛函，可以将机器学习方法很方便地与传统\textrm{DFT}计算结合起来使用\upcite{PRB94-245129_2016,PRL108-253002_2012,JCP139-224104_2013,IJQC116-819_2016,JCP148-241705_2018}。而且机器学习优化的泛函并不限于传统\textrm{DFT}的\textrm{Kohn-Sham}方程的交换-相关部分，也可以用于无轨道类型\textrm{(free-orbital)}的能量密度泛函。更值得注意的是，最近涌现了第三类应用机器方法来完成\textrm{DFT}计算的模式，其核心思路是直接预测电子密度\upcite{NC8-872_2017,CMS149-250_2018,arXiv1811006255_2018}，从而避免求解\textrm{Kohn-Sham}方程，本质上就是从\textrm{DFT}的\textrm{Hohenberg–Kohn}定理出发，但并非以寻找普适泛函为目标，而是通过机器学习获得材料的电子密度-势的映射关系，得到能量泛函后，对能量泛函求变分极小得到基态能量。图\ref{ML_DFT}给出了机器学习支持第一原理计算的三种不同形式。
\begin{figure}[h!]
\centering
\vspace*{-0.1in}
\includegraphics[width=4.2in, height=3.6in]{ML_DFT-2.png}
\caption{应用机器学习支持\textrm{DFT}的三种主要形式(引自文献\cite{NC8-872_2017}).}%
\label{ML_DFT}
\end{figure}

对于第一原理计算来说，机器学习还可于直接解决更为复杂的量子多体问题\upcite{CPC104-1_1997,Science355-602_2017,PRE98-033305_2018}，对于其\textrm{Schr\"odinger}方程\upcite{PRA96-042113_2017}的类紧束缚模型可以通过机器学习方法得到\textrm{Hamiltonian}\upcite{SR7-42669_2017}。

此外，机器学习在计算材料，特别是在高于电子尺度的物理学研究中的应用，还包括配分函数的确定\upcite{JCP149-044118_2018}、寻找相变和序参量\upcite{PRB94-195105_2016,NP13-431_2017,PRB96-205146_2017,SR7-8823_2017,PRB97-115453_2018}以及获得模型的格林函数\upcite{PRB90-155136_2014}等重要问题。机器学习这些领域的成功应用展示了数据挖掘技术在拓展材料科学研究前沿有着广阔的应用前景，可应用于多种尺度下材料学研究的各类系统和现象。材料研究领域应用机器学习方法是近年来才兴起的，但是总的来说，考虑到材料模拟计算的复杂性，大部分研究还停留在机器学习的起步层次\upcite{Nature559-547_2018,NPJCM3-54_2017,COSSMS21-167_2016,JM3-519_2017,JMR31-977_2016,MRSB41-399_2016,Science361-360_2018,JCP148-241401_2018,MRSB43-683_2018}。%，本部分主要讨论机器学习在典型领域的应用。
%\subsection{机器学习在第一原理材料研究中的应用}
%\subsubsection{新材料发现及稳定性}
%机器学习用于材料学研究的主要目标是通过数据驱动加速新的物质(主要是各类化合物)的发现。具体地说，通过机器学习，可以阐述材料的热力学稳定性，从而有效地预测化合物的形成能。\textrm{Curtarolo}和\textrm{Ceder}等是利用诸如\textrm{PCA}、线性回归等机器学习方法来预测晶体结构\upcite{PRL91-135503_2003}、形成能\upcite{MST16-296_2005}并优化高通量计算\upcite{NM5-641_2006}的先驱，、\textrm{Hautier}等应用机器学习方法，由\textrm{ICSD}的实验数据确定新的三元化合物的主要元素组成，并用高通量计算模拟验证\upcite{CM22-3762_2010}。\textrm{Saad}等用二元化合物为样本，介绍了机器学习的基本概念和监督学习与无监督学习降维技术\upcite{SR5-13285_2015}。\textrm{Patra}等应用偏倚神经网络遗传算法\textrm{(neural-newwork-biased genetic algorithm, NBGA)}加速特定物性材料的设计\upcite{ACSCS19-96_2017}，通过人工神经网络遗传算法的筛选优化，对模拟或实验样本数据按照类似生物进化的多代筛选，获得特定物性的提升。采用随机森林算法，预测\textrm{Heusler}合金时，仅以元素组分为特征向量(描述符)，得到很理想的结果，并且很快得到实验合成的验证\upcite{CM28-7324_2016}。\textrm{Faber}等对\textrm{ICSD}数据库中大量化合物的形成能应用\textrm{kernel ridge}回归，发现平均误差在0.1\textrm{eV/atom}的钾冰晶石化合物(\ch{ABC2D6})结构约90种\upcite{PRL117-135502_2016}。

%\textrm{Okamoto}在材料科学的化学组分研究中，应用\textrm{Bayesian}方法，只在6\%的搜索空间中就找到了嵌入金属锂的石墨烯稳定化学物\upcite{JPCA121-3299_2017}。

%预测晶体结构及其稳定性
%-------------------The Figure Of The Paper------------------
%\begin{figure}[h!]
%\centering
%\includegraphics[height=3.35in,width=2.85in,viewport=0 0 400 475,clip]{PbTe_Band_SO.eps}
%\hspace{0.5in}
%\includegraphics[height=3.35in,width=2.85in,viewport=0 0 400 475,clip]{EuTe_Band_SO.eps}
%\caption{\small Band Structure of PbTe (a) and EuTe (b).}%(与文献\upcite{EPJB33-47_2003}图1对比)
%\label{Pb:EuTe-Band_struct}
%\end{figure}

%-------------------The Equation Of The Paper-----------------
%\begin{equation}
%\varepsilon_1(\omega)=1+\frac2{\pi}\mathscr P\int_0^{+\infty}\frac{\omega'\varepsilon_2(\omega')}{\omega'^2-\omega^2}d\omega'
%\label{eq:magno-1}
%\end{equation}

%\begin{equation} 
%\begin{split}
%\varepsilon_2(\omega)&=\frac{e^2}{2\pi m^2\omega^2}\sum_{c,v}\int_{BZ}d{\vec k}\left|\vec e\cdot\vec M_{cv}(\vec k)\right|^2\delta [E_{cv}(\vec k)-\hbar\omega] \\
% &= \frac{e^2}{2\pi m^2\omega^2}\sum_{c,v}\int_{E_{cv}(\vec k=\hbar\omega)}\left|\vec e\cdot\vec M_{cv}(\vec k)\right|^2\dfrac{dS}{\nabla_{\vec k}E_{cv}(\vec k)}
% \end{split}
%\label{eq:magno-2}
%\end{equation}

%-------------------The Table Of The Paper----------------------
%\begin{table}[!h]
%\tabcolsep 0pt \vspace*{-12pt}
%%\caption{The representative $\vec k$ points contributing to $\sigma_2^{xy}$ of interband transition in EuTe around 2.5 eV.}
%\label{Table-EuTe_Sigma}
%\begin{minipage}{\textwidth}
%%\begin{center}
%\centering
%\def\temptablewidth{0.84\textwidth}
%\rule{\temptablewidth}{1pt}
%\begin{tabular*} {\temptablewidth}{|@{\extracolsep{\fill}}c|@{\extracolsep{\fill}}c|@{\extracolsep{\fill}}l|}

%-------------------------------------------------------------------------------------------------------------------------
%&Peak (eV)  & {$\vec k$}-point            &Band{$_v$} to Band{$_c$}  &Transition Orbital
%Components\footnote{波函数主要成分后的括号中，$5s$、$5p$和$5p$、$4f$、$5d$分别指碲和铕的原子轨道。} &Gap (eV)   \\ \hline
%-------------------------------------------------------------------------------------------------------------------------
%&2.35       &(0,0,0)         &33$\rightarrow$34    &$4f$(31.58)$5p$(38.69)$\rightarrow$$5p$      &2.142   \\% \cline{3-7}
%&       &(0,0,0)         &33$\rightarrow$34    &$4f$(31.58)$5p$(38.69)$\rightarrow$$5p$      &2.142   \\% \cline{3-7}
%-------------------------------------------------------------------------------------------------------------------------
%\end{tabular*}
%\rule{\temptablewidth}{1pt}
%\end{minipage}{\textwidth}
%\end{table}

%-------------------The Long Table Of The Paper--------------------
%\begin{small}
%%\begin{minipage}{\textwidth}
%%\begin{longtable}[l]{|c|c|cc|c|c|} %[c]指定长表格对齐方式
%\begin{longtable}[c]{|c|c|p{1.9cm}p{4.6cm}|c|c|}
%\caption{Assignment for the peaks of EuB$_6$}
%\label{tab:EuB6-1}\\ %\\长表格的caption中换行不可少
%\hline
%%
%--------------------------------------------------------------------------------------------------------------------------------
%\multicolumn{2}{|c|}{\bfseries$\sigma_1(\omega)$谱峰}&\multicolumn{4}{c|}{\bfseries部分重要能带间电子跃迁\footnotemark}\\ \hline
%\endfirsthead
%--------------------------------------------------------------------------------------------------------------------------------
%%
%\multicolumn{6}{r}{\it 续表}\\
%\hline
%--------------------------------------------------------------------------------------------------------------------------------
%标记 &峰位(eV) &\multicolumn{2}{c|}{有关电子跃迁} &gap(eV)  &\multicolumn{1}{c|}{经验指认} \\ \hline
%\endhead
%--------------------------------------------------------------------------------------------------------------------------------
%%
%\multicolumn{6}{r}{\it 续下页}\\
%\endfoot
%\hline
%--------------------------------------------------------------------------------------------------------------------------------
%%
%%\hlinewd{0.5$p$t}
%\endlastfoot
%--------------------------------------------------------------------------------------------------------------------------------
%%
%% Stuff from here to \endlastfoot goes at bottom of last page.
%%
%--------------------------------------------------------------------------------------------------------------------------------
%标记 &峰位(eV)\footnotetext{见正文说明。} &\multicolumn{2}{c|}{有关电子跃迁\footnotemark} &gap(eV) &\multicolumn{1}{c|}{经验指认\upcite{PRB46-12196_1992}}\\ \hline
%--------------------------------------------------------------------------------------------------------------------------------
%
%     &0.07 &\multicolumn{2}{c|}{电子群体激发$\uparrow$} &--- &电子群\\ \cline{2-5}
%\raisebox{2.3ex}[0pt]{$\omega_f$} &0.1 &\multicolumn{2}{c|}{电子群体激发$\downarrow$} &--- &体激发\\ \hline
%--------------------------------------------------------------------------------------------------------------------------------
%
%     &1.50 &\raisebox{-2ex}[0pt][0pt]{20-22(0,1,4)} &2$p$(10.4)4$f$(74.9)$\rightarrow$ &\raisebox{-2ex}[0pt][0pt]{1.47} &\\%\cline{3-5}
%     &1.50$^\ast$ & &2$p$(17.5)5$d_{\mathrm E}$(14.0)$\uparrow$ & &4$f$$\rightarrow$5$d_{\mathrm E}$\\ \cline{3-5}
%     \raisebox{2.3ex}[0pt][0pt]{$a$} &(1.0$^\dagger$) &\raisebox{-2ex}[0pt][0pt]{20-22(1,2,6)} &\raisebox{-2ex}[0pt][0pt]{4$f$(89.9)$\rightarrow$2$p$(18.7)5$d_{\mathrm E}$(13.9)$\uparrow$}\footnotetext{波函数主要成分后的括号中，2$s$、2$p$和5$p$、4$f$、5$d$、6$s$分别指硼和铕的原子轨道；5$d_{\mathrm E}$、5$d_{\mathrm T}$分别指铕的(5$d_{z^2}$，5$d_{x^2-y^2}$和5$d_{xy}$，5$d_{xz}$，5$d_{yz}$)轨道，5$d_{\mathrm{ET}}$(或5$d_{\mathrm{TE}}$)则指5个5$d$轨道成分都有，成分大的用脚标的第一个字母标示；2$ps$(或2$sp$)表示同时含有硼2$s$、2$p$轨道成分，成分大的用第一个字母标示。$\uparrow$和$\downarrow$分别标示$\alpha$和$\beta$自旋电子跃迁。} &\raisebox{-2ex}[0pt][0pt]{1.56} &激子跃迁。 \\%\cline{3-5}
%     &(1.3$^\dagger$) & & & &\\ \hline
%--------------------------------------------------------------------------------------------------------------------------------

%     & &\raisebox{-2ex}[0pt][0pt]{19-22(0,0,1)} &2$p$(37.6)5$d_{\mathrm T}$(4.5)4$f$(6.7)$\rightarrow$ & & \\\nopagebreak %\cline{3-5}
%     & & &2$p$(24.2)5$d_{\mathrm E}$(10.8)4$f$(5.1)$\uparrow$ &\raisebox{2ex}[0pt][0pt]{2.78} &a、b、c峰可能 \\ \cline{3-5}
%     & &\raisebox{-2ex}[0pt][0pt]{20-29(0,1,1)} &2$p$(35.7)5$d_{\mathrm T}$(4.8)4$f$(10.0)$\rightarrow$ & &包含有复杂的\\ \nopagebreak%\cline{3-5}
%     &2.90 & &2$p$(23.2)5$d_{\mathrm E}$(13.2)4$f$(3.8)$\uparrow$ &\raisebox{2ex}[0pt][0pt]{2.92} &强激子峰。$^{\ast\ast}$\\ \cline{3-5}
%$b$  &2.90$^\ast$ &\raisebox{-2ex}[0pt][0pt]{19-22(0,1,1)} &2$p$(33.9)4$f$(15.5)$\rightarrow$ & &B2$s$-2$p$的价带 \\ \nopagebreak%\cline{3-5}
%     &3.0 & &2$p$(23.2)5$d_{\mathrm E}$(13.2)4$f$(4.8)$\uparrow$ &\raisebox{2ex}[0pt][0pt]{2.94} &顶$\rightarrow$B2$s$-2$p$导\\ \cline{3-5}
%     & &12-15(0,1,2) &2$p$(39.3)$\rightarrow$2$p$(25.2)5$d_{\mathrm E}$(8.6)$\downarrow$ &2.83 &带底跃迁。\\ \cline{3-5}
%     & &14-15(1,1,1) &2$p$(42.5)$\rightarrow$2$p$(29.1)5$d_{\mathrm E}$(7.0)$\downarrow$ &2.96 & \\\cline{3-5}
%     & &13-15(0,1,1) &2$p$(40.4)$\rightarrow$2$p$(28.9)5$d_{\mathrm E}$(6.6)$\downarrow$ &2.98 & \\ \hline
%--------------------------------------------------------------------------------------------------------------------------------
%%\hline
%%\hlinewd{0.5$p$t}
%\end{longtable}
%%\end{minipage}{\textwidth}
%%\setlength{\unitlength}{1cm}
%%\begin{picture}(0.5,2.0)
%%  \put(-0.02,1.93){$^{1)}$}
%%  \put(-0.02,1.43){$^{2)}$}
%%\put(0.25,1.0){\parbox[h]{14.2cm}{\small{\\}}
%%\put(-0.25,2.3){\line(1,0){15}}
%%\end{picture}
%\end{small}

%------------------------------------直-接-插-入-文-件--------------------------------------------------------------------------------------
%\textcolor{red}{\textbf{直接插入文件}}:\verbatiminput{/home/jun_jiang/Documents/Latex_art_beamer/Daily_WORKS/Report-2020_model.tex} %为保险:~选用文件名绝对路径
%\textcolor{red}{\textbf{备忘录}}:\verbatiminput{/home/jun_jiang/Documents/备忘录.txt}
%---------------------------------------------------------------------------------------------------------------------------------------------%

%-------------------------------------------------------------------------Thanks------------------------------------------------------------------------------------------------
%\newpage %%
%\newpage %%
%\thispagestyle{fancy}   % 首页插入页眉页脚 
%\section{致谢}
%致谢内容
%-----------------------------------------------------------------------------------------------------------------------------------------------------------------------%

%--------------------------------------------------------------------------The Biblography of The Paper-----------------------------------------------------------------%
%\newpage																				%
%-----------------------------------------------------------------------------------------------------------------------------------------------------------------------%
%\begin{thebibliography}{99}																		%
%%\bibitem{PRL58-65_1987}H.Feil, C. Haas, {\it Phys. Rev. Lett.} {\bf 58}, 65 (1987).											%
%	\bibitem{kp-method} \textrm{Zhenxi Pan, Yong Pan, Jun Jiang$^{\ast}$, Liutao Zhao}, \textrm{High-Throughput Electronic Band Structure Calculations for Hexaborides}, \textit{Intelligent Computing}, \textbf{Springer}, \textbf{P.386-395}, (2019).%
%	\bibitem{PAW-dataset} \textrm{姜骏}，\textrm{PAW原子数据集的构造与检验}, \textit{中国化学会第十二届全国量子化学会议论文摘要集}，\textbf{太原}，(2014).
%\end{thebibliography}																			%
%-----------------------------------------------------------------------------------------------------------------------------------------------------------------------%
\phantomsection\addcontentsline{toc}{section}{Bibliography} %直接调用\addcontentsline命令可能导致超链指向不准确,一般需要在之前调用一次\phantomsection命令加以修正%
%\bibliography{../ref/Myref_from_2013}   %
\bibliography{../ref/Myref}   %
\bibliographystyle{../ref/mybib} %% 接近ieeert样式
%%%%%%%%%%%%%%%%%%%%%%%%%%%%      \bibliographystyle         %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%      LaTeX 参考文献标准选项及其样式共有以下8种：                                %%%%%%%%
% plain，按字母的顺序排列，比较次序为作者、年度和标题.
% unsrt，样式同plain，只是按照引用的先后排序.
% alpha，用作者名首字母+年份后两位作标号，以字母顺序排序.
% abbrv，类似plain，将月份全拼改为缩写，更显紧凑.
% ieeetr，国际电气电子工程师协会期刊样式.
% acm，美国计算机学会期刊样式.
% siam，美国工业和应用数学学会期刊样式.
% apalike，美国心理学学会期刊样式.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  \nocite{*}																				%
%-----------------------------------------------------------------------------------------------------------------------------------------------------------------------%

\clearpage     %\end{CJK} 前加上\clearpage是CJK的要求
%\end{CJK*}
\end{document}
